function [trainingInfo] = training_CaseI(training, seed, reconfiguration)
% TRAINING_CASEI - Train a DDPG agent with RNN for a given environment setup
%
% Inputs:
%   training        - logical flag or parameter for environment
%   seed            - random seed for reproducibility
%   reconfiguration - environment reconfiguration flag
%
% Output:
%   trainingInfo    - training statistics from the RL training session

    %% Environment Setup
    env = Copy_of_environment;
    env.training = training;
    env.reconfiguration = reconfiguration;

    rng(seed);

    %% Training Options
    trainOpts = rlTrainingOptions( ...
        'MaxEpisodes',                20000, ...
        'MaxStepsPerEpisode',         500, ...
        'StopTrainingCriteria',       "AverageReward", ...
        'StopTrainingValue',          10000, ...
        'ScoreAveragingWindowLength', 24, ...
        'SaveAgentCriteria',          "AverageReward", ...
        'SaveAgentValue',             100, ...
        'SaveAgentDirectory',         "savedAgents", ...
        'Verbose',                    false, ...
        'Plots',                      "training-progress",...
        'ExecutionEnvironment',       "gpu");      % <-- GPU enabled);


    %% Observation and Action Info
    obsInfo = getObservationInfo(env);
    actInfo = getActionInfo(env);

    %% ===== ACTOR NETWORK (Recurrent) =====
    scaleLayer = scalingLayer('Name', 'scale', 'Scale', 0.5, 'Bias', 0.5);

    actorNet = [
        sequenceInputLayer(obsInfo.Dimension(1), 'Name', 'input')
        fullyConnectedLayer(128, 'Name', 'fc1')
        reluLayer('Name', 'relu1')
        fullyConnectedLayer(128, 'Name', 'fc2')
        reluLayer('Name', 'relu2')
        lstmLayer(128, 'OutputMode', 'sequence', 'Name', 'lstm')
        layerNormalizationLayer('Name', 'layernorm')
        dropoutLayer(0.2, 'Name', 'dropout') % 20% dropout
        fullyConnectedLayer(prod(actInfo.Dimension), 'Name', 'fc_out')
        tanhLayer('Name', 'tanh')
        scaleLayer
    ];

    actor = rlContinuousDeterministicActor(actorNet, obsInfo, actInfo);

    %% ===== CRITIC NETWORK (Recurrent) =====
    statePath = [
        sequenceInputLayer(obsInfo.Dimension(1), 'Name', 'state')
        fullyConnectedLayer(128, 'Name', 'fc1_state')
    ];

    actionPath = [
        sequenceInputLayer(prod(actInfo.Dimension), 'Name', 'action')
        fullyConnectedLayer(128, 'Name', 'fc1_action')
    ];

    commonPath = [
        concatenationLayer(1, 2, 'Name', 'concat')
        reluLayer('Name', 'relu1')
        fullyConnectedLayer(128, 'Name', 'fc2')
        reluLayer('Name', 'relu2')
        lstmLayer(128, 'OutputMode', 'sequence', 'Name', 'lstm')
        layerNormalizationLayer('Name', 'layernorm')
        dropoutLayer(0.2, 'Name', 'dropout') % 20% dropout
        fullyConnectedLayer(1, 'Name', 'value')
    ];

    criticNet = layerGraph(statePath);
    criticNet = addLayers(criticNet, actionPath);
    criticNet = addLayers(criticNet, commonPath);
    criticNet = connectLayers(criticNet, 'fc1_state', 'concat/in1');
    criticNet = connectLayers(criticNet, 'fc1_action', 'concat/in2');

    critic = rlQValueFunction(criticNet, obsInfo, actInfo);

    %% ===== AGENT OPTIONS =====
    agentOpts = rlDDPGAgentOptions( ...
        'SampleTime',            1, ...
        'DiscountFactor',        0.99, ...
        'TargetSmoothFactor',    1e-3, ...
        'ExperienceBufferLength',1e6, ...
        'MiniBatchSize',         64, ...
        'SequenceLength',        2, ...
        'NumStepsToLookAhead',   1);

    % Optimizer Settings
    agentOpts.CriticOptimizerOptions.LearnRate         = 1e-3;
    agentOpts.CriticOptimizerOptions.GradientThreshold = 1000;
    agentOpts.ActorOptimizerOptions.LearnRate          = 1e-3;
    agentOpts.ActorOptimizerOptions.GradientThreshold  = 1000;

    % Exploration Noise
    agentOpts.NoiseOptions.StandardDeviation          = 0.35;
    agentOpts.NoiseOptions.StandardDeviationDecayRate = 1e-4;
    agentOpts.NoiseOptions.Mean                       = 0;

    %% ===== CREATE AND TRAIN AGENT =====
    agent = rlDDPGAgent(actor, critic, agentOpts);
    agent.UseExplorationPolicy = true;

    trainingInfo = train(agent, env, trainOpts);
end
